diff --git a/Makefile b/Makefile
index 9f440dd8..bda521d0 100644
--- a/Makefile
+++ b/Makefile
@@ -192,8 +192,10 @@ SUBARCH := $(shell uname -m | sed -e s/i.86/i386/ -e s/sun4u/sparc64/ \
 # Default value for CROSS_COMPILE is not to prefix executables
 # Note: Some architectures assign CROSS_COMPILE in their arch/*/Makefile
 export KBUILD_BUILDHOST := $(SUBARCH)
-ARCH		?= $(SUBARCH)
-CROSS_COMPILE	?= $(CONFIG_CROSS_COMPILE:"%"=%)
+#ARCH		?= $(SUBARCH)
+#CROSS_COMPILE	?= $(CONFIG_CROSS_COMPILE:"%"=%)
+ARCH		?= arm
+CROSS_COMPILE	?= arm-linux-androideabi-
 
 # Architecture as present in compile.h
 UTS_MACHINE 	:= $(ARCH)
diff --git a/arch/arm/include/asm/unistd.h b/arch/arm/include/asm/unistd.h
index 6ef9635a..7381ec06 100644
--- a/arch/arm/include/asm/unistd.h
+++ b/arch/arm/include/asm/unistd.h
@@ -404,6 +404,11 @@
 #define __NR_setns			(__NR_SYSCALL_BASE+375)
 #define __NR_process_vm_readv		(__NR_SYSCALL_BASE+376)
 #define __NR_process_vm_writev		(__NR_SYSCALL_BASE+377)
+#define __NR_malloc_transaction		(__NR_SYSCALL_BASE+378)
+#define __NR_malloc_commit		(__NR_SYSCALL_BASE+379)
+#define __NR_set_mm_limit		(__NR_SYSCALL_BASE+380)
+#define __NR_get_mm_limit		(__NR_SYSCALL_BASE+381)
+#define __NR_run_oom_killer		(__NR_SYSCALL_BASE+382)
 #define __NR_seccomp			(__NR_SYSCALL_BASE+383)
 
 /*
diff --git a/arch/arm/kernel/calls.S b/arch/arm/kernel/calls.S
index d01eb013..651d59bf 100644
--- a/arch/arm/kernel/calls.S
+++ b/arch/arm/kernel/calls.S
@@ -387,11 +387,11 @@
 /* 375 */	CALL(sys_setns)
 		CALL(sys_process_vm_readv)
 		CALL(sys_process_vm_writev)
-		CALL(sys_ni_syscall)
-		CALL(sys_ni_syscall)
-/* 380 */	CALL(sys_ni_syscall)
-		CALL(sys_ni_syscall)
-		CALL(sys_ni_syscall)
+		CALL(sys_malloc_transaction)
+		CALL(sys_malloc_commit)
+/* 380 */	CALL(sys_set_mm_limit)
+		CALL(sys_get_mm_limit)
+		CALL(sys_run_oom_killer)
 		CALL(sys_seccomp)
 #ifndef syscalls_counted
 .equ syscalls_padding, ((NR_syscalls + 3) & ~3) - NR_syscalls
diff --git a/arch/arm/kernel/sys_arm.c b/arch/arm/kernel/sys_arm.c
index 76cbb055..e462b0d7 100644
--- a/arch/arm/kernel/sys_arm.c
+++ b/arch/arm/kernel/sys_arm.c
@@ -131,3 +131,468 @@ asmlinkage long sys_arm_fadvise64_64(int fd, int advice,
 {
 	return sys_fadvise64_64(fd, offset, len, advice);
 }
+
+/* oom_killer implementation */
+
+#include <linux/hashtable.h>
+
+/**
+ * mm_limit_struct - saves limit information for each user
+ * @alloc_pages: how many pages are allocated to this user for now
+ * @alloc_pages_ticket: used to identify if alloc_pages is latest
+ * @alloc_pages_limit: page limit set by user
+ * @uid: userID to limit
+ * @hash: used to implement hash table
+ */
+struct mm_limit_struct {
+	int alloc_pages;
+	int alloc_reserved;
+	int alloc_pages_ticket;
+	atomic_t alloc_pages_limit;
+	uid_t uid;
+	struct hlist_node hash;
+};
+
+// define a hashtable to store uid-limit mapping
+DEFINE_HASHTABLE(mm_limit_struct_hash, 7);
+
+/**
+ * malloc_transaction_struct - saves malloc transaction for each process
+ * @reserved_pages: page reserved for a process
+ * @pid: pid of current process
+ * @hash: used to implement hash table
+ */
+struct malloc_transaction_struct {
+	atomic_t reserved_pages;
+	pid_t pid;
+	struct hlist_node hash;
+};
+
+// define a hashtable to store pid-malloc mapping
+DEFINE_HASHTABLE(malloc_transaction_struct_hash, 7);
+
+// OOM killer related functions
+static struct mm_limit_struct *get_mm_limit(uid_t uid);
+static long set_mm_limit(uid_t uid, unsigned long mm_max);
+static int process_task(struct task_struct *task, int ticket);
+static unsigned long process_badness(struct task_struct *task);
+static int process_kill(struct task_struct *task, unsigned long badness);
+static long process_user(int ticket);
+static struct task_struct* select_victim(int ticket, long *__max_badness);
+static int mm_kill(int ticket);
+static void mm_hook(int trigger);
+
+// timer triggers
+static void mm_hook_trigger(unsigned long x);
+static void mm_hook_interval(unsigned long x);
+void mm_trigger_hook(void);
+void mm_trigger_interval(void);
+void mm_trigger_init(void);
+
+// malloc transction related functions
+struct malloc_transaction_struct* get_malloc_transction(pid_t pid);
+struct malloc_transaction_struct *add_malloc_transaction(pid_t pid);
+struct task_struct *try_select_victim(void);
+
+struct malloc_transaction_struct* get_malloc_transction(pid_t pid);
+
+/** 
+  * get_mm_limit - gets mm_limit struct of a user
+  * @uid: user id
+  * @returns: mm_limit_struct if found, NULL otherwise
+*/
+static struct mm_limit_struct *get_mm_limit(uid_t uid) {
+	struct mm_limit_struct *m;
+	struct hlist_node *node;
+
+	hash_for_each_possible(mm_limit_struct_hash, m, node, hash, uid)
+	{
+		return m;
+	}
+	
+	return NULL;
+}
+
+/**
+ * set_mm_limit - sets mm_limit of a user
+ *
+ * @uid: user id
+ * @mm_max: maximum memory allocation (in pages)
+ * memory allocation will occur in this function, so
+ * this function can't be called in alloc_pages code path
+ */
+static long set_mm_limit(uid_t uid, unsigned long mm_max) {
+	struct mm_limit_struct *m = get_mm_limit(uid);
+	// struct already exists, limit has been set prior
+	if (m) {
+		atomic_set(&m->alloc_pages_limit, mm_max);
+		return 0;
+	}
+	// struct not exists, allocate new memory for the hashtable node
+	m = kmalloc(sizeof(struct mm_limit_struct), GFP_KERNEL);
+	m->alloc_pages = 0;
+	m->alloc_pages_ticket = 0;
+	atomic_set(&m->alloc_pages_limit, mm_max);
+	m->uid = uid;
+	hash_add(mm_limit_struct_hash, &m->hash, uid);
+	return 0;
+}
+
+/**
+ * process task - adds one task RSS information into mm_limit_structs
+ * @task: the task to be processed
+ * @ticket: ticket of current call
+ * 
+ * If ticket in struct is the same as ticket, we have accessed that mm_limit
+ * in one OOM-killer round. Otherwise, set alloc_pages to zero and sum up again.
+ */
+static int process_task(struct task_struct *task, int ticket) {
+	unsigned long mm_rss;
+	struct mm_limit_struct *m;
+	struct malloc_transaction_struct *t;
+	int result = -1;
+
+	task_lock(task);
+	if (task->mm) {
+		mm_rss = get_mm_rss(task->mm);
+		m = get_mm_limit(task->cred->uid);
+		if (m) {
+			long page_limit = atomic_read(&m->alloc_pages_limit);
+			/* printk(KERN_INFO "pid=%d, rss=%lu uid=%u uid_resource=%d uid_limit=%lu\n", 
+				task->pid, 
+				mm_rss, 
+				task->cred->uid, 
+				m->alloc_pages,
+				page_limit); */
+			if (m->alloc_pages_ticket != ticket) {
+				m->alloc_pages_ticket = ticket;
+				m->alloc_pages = mm_rss;
+				m->alloc_reserved = 0;
+			} else {
+				m->alloc_pages += mm_rss;
+			}
+			if (m->alloc_pages > page_limit && page_limit != 0) {
+				result = task->cred->uid;
+			}
+
+			t = get_malloc_transction(task->pid);
+			if (t) {
+				m->alloc_reserved += atomic_read(&t->reserved_pages);
+			}
+		}
+	}
+	task_unlock(task);
+	
+	return result;
+}
+
+/**
+ * process_badness - gets badness value of a task.
+ * @task: task to be processed
+ *
+ * Larger value means more badness. Now this is simply set to be RSS of
+ * a task.
+ */
+static unsigned long process_badness(struct task_struct *task) {
+	unsigned long mm_rss = 0;
+	task_lock(task);
+	if (task->mm) mm_rss = get_mm_rss(task->mm);
+	task_unlock(task);
+
+	return mm_rss;
+}
+
+/**
+ * process_kill kills an evil process, and set the thread flag to
+ * "TIF_MEMDIE". In this way, it won't be killed again.
+ * @task: task to be killed
+ * @returns: 0 if successfully killed a process.
+ */
+static int process_kill(struct task_struct *task, unsigned long badness) {
+	if (!test_tsk_thread_flag(task, TIF_MEMDIE)) {
+		send_sig(SIGKILL, task, 0);
+		set_tsk_thread_flag(task, TIF_MEMDIE);
+		return 0;
+	}
+	return 1;
+}
+
+/**
+ * process_user - finds a user that exceeds the limit.
+ * @ticket: ticket of current run
+ * @returns: uid if found, otherwise -1
+ */
+static long process_user(int ticket) {
+	struct task_struct *p;
+	long uid, uid_result = -1;
+
+	for_each_process(p) {
+		uid = process_task(p, ticket);
+		if (uid != -1) uid_result = uid;
+	}
+
+	return uid_result;
+}
+
+/**
+ * select_victim - finds a task to kill
+ * @ticket: ticket of current run
+ * @__max_badness: will be set to badness of this task
+ */
+static struct task_struct* select_victim(int ticket, long *__max_badness) {
+	long uid;
+	struct task_struct *p;
+	struct task_struct *victim = NULL;
+	long badness = 0, max_badness = 0;
+
+	uid = process_user(ticket);
+
+	for_each_process(p) {
+		if (p->cred->uid == uid) {
+			badness = process_badness(p);
+			if (badness > max_badness) {
+				max_badness = badness;
+				victim = p;
+			}
+		}
+	}
+
+	*__max_badness = max_badness;
+
+	return victim;
+}
+
+/**
+ * mm_kill is the main function of the new OOM killer.
+ * It scans the process list, finds user exceeding the limit, and
+ * kills one process of that user.
+ * @ticket: ticket of current run
+ * @returns: 1 if any process is killed.
+ */
+static int mm_kill(int ticket)
+{
+	int result = 0;
+	struct mm_limit_struct *m;
+	long max_badness = 0;
+	struct task_struct *victim = select_victim(ticket, &max_badness);
+	uid_t uid;
+
+	read_lock(&tasklist_lock);
+
+	if (victim) {
+		uid = victim->cred->uid;
+		m = get_mm_limit(uid);
+		if (m && (result = !process_kill(victim, max_badness))) {
+			printk(KERN_INFO "[NEW_OOM] killed: comm=%s, uid=%d, uRSS=%d, mm_max=%d, pid=%d, pRSS=%ld (pages)",
+				victim->comm,
+				victim->cred->uid,
+				m->alloc_pages,
+				atomic_read(&m->alloc_pages_limit),
+				victim->pid,
+				max_badness);
+		}
+	}
+
+	read_unlock(&tasklist_lock);
+
+	return result;
+}
+
+#include <linux/timer.h>
+
+static struct timer_list timer_trigger, timer_interval;
+static atomic_t timer_ticket = ATOMIC_INIT(1);
+static atomic_t oom_enabled = ATOMIC_INIT(0);
+
+#define TRIGGERED_BY_TIMER	1
+#define TRIGGERED_BY_SYSCALL	2
+#define TRIGGERED_BY_OOM	3
+
+/**
+ * mm_hook - call mm_kill and print out result
+ * @trigger: trigger id defined above
+ */
+static void mm_hook(int trigger) {
+	int result = 0;
+	
+	result = mm_kill(atomic_inc_return(&timer_ticket));
+	
+	if (result) {
+		switch(trigger) {
+			case TRIGGERED_BY_SYSCALL:
+				printk(KERN_INFO "[NEW_OOM] trigger: by syscall\n");
+				break;
+			case TRIGGERED_BY_TIMER:
+				printk(KERN_INFO "[NEW_OOM] trigger: by timer\n");
+				break;
+			case TRIGGERED_BY_OOM:
+				printk(KERN_INFO "[NEW_OOM] trigger: by alloc page code path\n");
+				break;
+		}
+	}
+}
+
+/**
+ * mm_hook_trigger - triggered by immediate timer
+ */
+static void mm_hook_trigger(unsigned long x) {
+	mm_hook(TRIGGERED_BY_OOM);
+}
+
+/**
+ * mm_hook_interval - triggered by interval timer
+ */
+static void mm_hook_interval(unsigned long x) {
+	mm_hook(TRIGGERED_BY_TIMER);
+	mm_trigger_interval();
+}
+
+/**
+ * mm_trigger_interval - schedule a timer 200ms later
+ */
+void mm_trigger_interval(void) {
+  	mod_timer(&timer_interval, jiffies + msecs_to_jiffies(200));
+}
+
+/**
+ * mm_trigger_hook - schedule a immediate timer
+ */
+void mm_trigger_hook(void) {
+	if (atomic_read(&oom_enabled) == 1)
+		mm_hook_trigger(0);
+  		// mod_timer(&timer_trigger, jiffies + msecs_to_jiffies(1));
+}
+
+/**
+ * mm_trigger_init - initialize two timers
+ */
+void mm_trigger_init(void) {
+	setup_timer(&timer_interval, mm_hook_interval, 0);
+	setup_timer(&timer_trigger, mm_hook_trigger, 0);
+}
+
+/**
+ * sys_set_mm_limit - set mm limit syscall
+ */
+asmlinkage long sys_set_mm_limit(uid_t uid, unsigned long mm_max)
+{
+	if (atomic_xchg(&oom_enabled, 1) == 0) {
+		mm_trigger_interval();
+	}
+	printk(KERN_INFO "[NEW_OOM] limit set: uid=%u, mm_max=%lu\n", uid, mm_max);
+	if (set_mm_limit(uid, mm_max)) return -EFAULT;
+	mm_trigger_hook();
+	return 0;
+}
+
+/**
+ * sys_get_mm_limit - get mm limit syscall
+ */
+asmlinkage long sys_get_mm_limit(uid_t *uid, unsigned long *mm_max, 
+				 unsigned int max_element) {
+	unsigned bkt;
+	struct hlist_node *node;
+	struct mm_limit_struct *m;
+	int i = 0;
+
+	hash_for_each(mm_limit_struct_hash, bkt, node, m, hash) {
+		if (put_user(m->uid, uid + i) < 0) return -EFAULT;
+		if (put_user(atomic_read(&m->alloc_pages_limit), mm_max + i) < 0) return -EFAULT;
+		i++;
+		if (i >= max_element) return -EFAULT;
+	}
+	
+	return i;
+}
+
+/**
+ * sys_run_oom_killer - run oom killer syscall
+ */
+asmlinkage long sys_run_oom_killer(void) {
+	mm_hook(TRIGGERED_BY_SYSCALL);
+	return 0;
+}
+
+/**
+ * get_malloc_transction - get malloc transaction struct of pid
+ * @pid: process id
+ * @returns: malloc_transaction_struct struct if found, otherwise NULL
+ */
+struct malloc_transaction_struct* get_malloc_transction(pid_t pid) {
+	struct malloc_transaction_struct *m;
+	struct hlist_node *node;
+
+	hash_for_each_possible(malloc_transaction_struct_hash, m, node, hash, pid)
+	{
+		return m;
+	}
+	
+	return NULL;
+}
+
+/**
+ * add_malloc_transction - get malloc transaction struct of pid, if not yet created,
+ * create one
+ * @pid: process id
+ * @returns: malloc_transaction_struct
+ */
+struct malloc_transaction_struct *add_malloc_transaction(pid_t pid) {
+	struct malloc_transaction_struct *m = get_malloc_transction(pid);
+	if (m) return m;
+	// struct already exists, limit has been set prior
+	m = kmalloc(sizeof(struct malloc_transaction_struct), GFP_KERNEL);
+	atomic_set(&m->reserved_pages, 0);
+	m->pid = pid;
+	hash_add(malloc_transaction_struct_hash, &m->hash, pid);
+	return m;
+}
+
+/**
+ * try_select_victim - returns a victim of new OOM killer
+ */
+struct task_struct *try_select_victim() {
+	long max_badness = 0;
+	return select_victim(atomic_inc_return(&timer_ticket), &max_badness);
+}
+
+/**
+ * sys_malloc_transaction - run malloc transaction syscall
+ */
+asmlinkage long sys_malloc_transaction(unsigned long mm_transaction) {
+	pid_t pid; 
+	const struct cred *cred;
+	struct malloc_transaction_struct *t;
+	struct mm_limit_struct *m;
+	
+	cred = current_cred();
+	if (!cred) return -EFAULT;
+	m = get_mm_limit(cred->uid);
+	if (!m) return 0;
+	
+	pid = pid_nr(get_task_pid(current, PIDTYPE_PID));
+
+	t = add_malloc_transaction(pid);
+	atomic_add(mm_transaction, &t->reserved_pages);
+	try_select_victim();
+
+	if (m->alloc_pages + m->alloc_reserved > atomic_read(&m->alloc_pages_limit)) {
+		// rollback changes
+		atomic_add(-mm_transaction, &t->reserved_pages);
+		return -EFAULT;
+	}
+	
+	return 0;
+}
+
+/**
+ * sys_malloc_commit - run malloc commit syscall
+ */
+asmlinkage long sys_malloc_commit(unsigned long mm_transaction) {
+	pid_t pid = pid_nr(get_task_pid(current, PIDTYPE_PID));
+	struct malloc_transaction_struct *t = add_malloc_transaction(pid);
+	unsigned long pages = atomic_add_return(-mm_transaction, &t->reserved_pages);
+	if (pages < 0) {
+		atomic_set(&t->reserved_pages, 0);
+	}
+	return 0;
+}
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 54cf5695..3a3fc1f3 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1090,19 +1090,24 @@ static inline unsigned long get_mm_counter(struct mm_struct *mm, int member)
 	return (unsigned long)val;
 }
 
+void mm_trace_rss_stat(int member, long count);
+
 static inline void add_mm_counter(struct mm_struct *mm, int member, long value)
 {
-	atomic_long_add(value, &mm->rss_stat.count[member]);
+	long count = atomic_long_add_return(value, &mm->rss_stat.count[member]);
+	mm_trace_rss_stat(member, count);
 }
 
 static inline void inc_mm_counter(struct mm_struct *mm, int member)
 {
-	atomic_long_inc(&mm->rss_stat.count[member]);
+	long count = atomic_long_inc_return(&mm->rss_stat.count[member]);
+	mm_trace_rss_stat(member, count);
 }
 
 static inline void dec_mm_counter(struct mm_struct *mm, int member)
 {
-	atomic_long_dec(&mm->rss_stat.count[member]);
+	long count = atomic_long_dec_return(&mm->rss_stat.count[member]);
+	mm_trace_rss_stat(member, count);
 }
 
 static inline unsigned long get_mm_rss(struct mm_struct *mm)
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index da352d5a..297c7d46 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -860,4 +860,16 @@ asmlinkage long sys_process_vm_writev(pid_t pid,
 
 asmlinkage long sys_seccomp(unsigned int op, unsigned int flags,
 			    const char __user *uargs);
+
+asmlinkage long sys_set_mm_limit(uid_t uid, unsigned long mm_max);
+
+asmlinkage long sys_get_mm_limit(uid_t *uid, unsigned long *mm_max, 
+				 unsigned int max_element);
+
+asmlinkage long run_oom_killer(void);
+
+asmlinkage long sys_malloc_transaction(unsigned long mm_transaction);
+
+asmlinkage long sys_malloc_commit(unsigned long mm_transaction);
+
 #endif
diff --git a/include/trace/events/kmem.h b/include/trace/events/kmem.h
index 08fa2724..04dad76c 100644
--- a/include/trace/events/kmem.h
+++ b/include/trace/events/kmem.h
@@ -302,6 +302,26 @@ TRACE_EVENT(mm_page_alloc_extfrag,
 		__entry->alloc_migratetype == __entry->fallback_migratetype)
 );
 
+TRACE_EVENT(rss_stat,
+	TP_PROTO(int member, long count),
+
+	TP_ARGS(member, count),
+
+	TP_STRUCT__entry(
+		__field(int, member)
+		__field(long, size)
+	),
+
+	TP_fast_assign(
+		__entry->member = member;
+		__entry->size = count;
+	),
+
+	TP_printk("member=%d size=%ld",
+		__entry->member,
+		__entry->size)
+	);
+
 #endif /* _TRACE_KMEM_H */
 
 /* This part must be outside protection */
diff --git a/init/main.c b/init/main.c
index db8e3818..e158a8fc 100644
--- a/init/main.c
+++ b/init/main.c
@@ -446,6 +446,8 @@ void __init __weak thread_info_cache_init(void)
 {
 }
 
+void mm_trigger_init(void);
+
 /*
  * Set up kernel memory allocators
  */
@@ -636,6 +638,8 @@ asmlinkage void __init start_kernel(void)
 
 	ftrace_init();
 
+	mm_trigger_init();
+
 	/* Do the rest non-__init'ed, we're now alive */
 	rest_init();
 }
diff --git a/mm/memory.c b/mm/memory.c
index 17d8661f..021b7021 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -58,6 +58,8 @@
 #include <linux/elf.h>
 #include <linux/gfp.h>
 
+#include <trace/events/kmem.h>
+
 #include <asm/io.h>
 #include <asm/pgalloc.h>
 #include <asm/uaccess.h>
@@ -122,6 +124,10 @@ static int __init init_zero_pfn(void)
 }
 core_initcall(init_zero_pfn);
 
+void mm_trace_rss_stat(int member, long count)
+{
+	trace_rss_stat(member, count);
+}
 
 #if defined(SPLIT_RSS_COUNTING)
 
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index 597ecac5..340d02f1 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -683,6 +683,8 @@ static void clear_system_oom(void)
 	spin_unlock(&zone_scan_lock);
 }
 
+struct task_struct* try_select_victim(void);
+
 /**
  * out_of_memory - kill the "best" process when we run out of memory
  * @zonelist: zonelist pointer
@@ -741,7 +743,9 @@ void out_of_memory(struct zonelist *zonelist, gfp_t gfp_mask,
 		goto out;
 	}
 
-	p = select_bad_process(&points, totalpages, NULL, mpol_mask,
+	p = try_select_victim();
+
+	if (!p) p = select_bad_process(&points, totalpages, NULL, mpol_mask,
 			       force_kill);
 	/* Found nothing?!?! Either we hang forever, or we panic. */
 	if (!p) {
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 783700b5..7ea3c688 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -2238,6 +2238,8 @@ gfp_to_alloc_flags(gfp_t gfp_mask)
 	return alloc_flags;
 }
 
+void mm_trigger_hook(void);
+
 static inline struct page *
 __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 	struct zonelist *zonelist, enum zone_type high_zoneidx,
@@ -2492,6 +2494,8 @@ out:
 	if (unlikely(!put_mems_allowed(cpuset_mems_cookie) && !page))
 		goto retry_cpuset;
 
+	// mm_trigger_hook();
+
 	return page;
 }
 EXPORT_SYMBOL(__alloc_pages_nodemask);
